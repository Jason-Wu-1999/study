https://blog.csdn.net/liyuxing6639801/article/details/105546522?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163265178216780262577775%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=163265178216780262577775&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-2-105546522.first_rank_v2_pc_rank_v29&utm_term=mysql%E9%9D%A2%E8%AF%95&spm=1018.2226.3001.4187

## 引擎

### **Mysql 中 MyISAM 和 InnoDB 的区别有哪些？**

**区别：**

1. InnoDB支持事务，MyISAM不支持

   对于InnoDB每一条SQL语言都默认封装成事务，自动提交，这样会影响速度，所以最好把多条SQL语言放在begin和commit之间，组成一个事务；

2. InnoDB支持外键，而MyISAM不支持。对一个包含外键的InnoDB表转为MYISAM会失败；

3. InnoDB是聚集索引，数据文件是和索引绑在一起的，必须要有主键，通过主键索引效率很高。

   但是非聚簇索引（辅助索引）需要两次查询，先查询到主键，然后再通过主键查询到数据。因此主键不应该过大，因为主键太大，其他索引也都会很大。

   而MyISAM是非聚集索引，数据文件是分离的，索引保存的是数据文件的指针。主键索引和辅助索引是独立的。

4. InnoDB不保存表的具体行数，执行select count(*) from table时需要全表扫描。而MyISAM用一个变量保存了整个表的行数，执行上述语句时只需要读出该变量即可，速度很快；

5. Innodb不支持全文索引，而MyISAM支持全文索引，查询效率上MyISAM要高；

**如何选择：**

1. 是否要支持事务，如果要请选择innodb，如果不需要可以考虑MyISAM；
2. 如果表中绝大多数都只是读查询，可以考虑MyISAM，如果既有读写也挺频繁，请使用InnoDB
3. 系统奔溃后，MyISAM恢复起来更困难，能否接受；
4. MySQL5.5版本开始Innodb已经成为Mysql的默认引擎(之前是MyISAM)，说明其优势是有目共睹的，如果你不知道用什么，那就用InnoDB，至少不会差。



### 执行一条查询语句

#### 	连接器

- TCP三次握手，建立TCP连接
- 验证用户名和密码
- 获取用户权限
  - 这就意味着，一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限
    		

连接可分为长连接和短连接
			如何解决长连接占用内存的问题
			1、定期断开长连接
			2、客户端主动重置连接

#### 	查询缓存

- 如果是查询语句，会先去查询缓存 key是sql语句 value是结果		
- 但是每次表有更新操作就会清空缓存	
- MySQL8.0之后直接将查询缓存删掉了

#### 	解析SQL

- 词法分析
  	识别关键字 构建出SQL语法树
    	将 select * 中的 * 符号扩展为表上的所有列。	
- 语法分析
  	解析语法是否正确

#### 	执行SQL

> 执行器  调用引擎层接口执行 将参数传给引擎层   ，类似于 control层和model层

- 预处理器
  	判断表或字段是否存在
- 优化
  	优化器主要负责将 SQL 查询语句的执行方案确定下来
- 执行
  	根据执行计划执行 SQL 查询语句，从存储引擎读取记录，返回给客户端


### 更新语句的话会涉及redo log和binlog日志

1. 首先查找出要更新的行
2. 将该数据页读入内存
3. 在内存中对改行进行修改
4. 写redobuffer日志（prepare状态）
5. 写binlog日志
6. 提交事务
7. redolog刷盘（commit状态）



### **SELECT语句 - 执行顺序：**

**FROM**
<表名> # 选取表，将多个表数据通过笛卡尔积变成一个表。
**ON**
<筛选条件> # 对笛卡尔积的虚表进行筛选
**JOIN** <join, left join, right join...>
<join表> # 指定join，用于添加数据到on之后的虚表中，例如left join会将左表的剩余数据添加到虚表中
**WHERE**
<where条件> # 对上述虚表进行筛选
**GROUP BY**
<分组条件> # 分组
<SUM()等聚合函数> # 用于having子句进行判断，在书写上这类聚合函数是写在having判断里面的
**HAVING**
<分组筛选> # 对分组后的结果进行聚合筛选
**SELECT**
<返回数据列表> # 返回的单列必须在group by子句中，聚合函数除外
**DISTINCT**
\# 数据除重
**ORDER BY**
<排序条件> # 排序
**LIMIT**
<行数限制>



## 事务

### 并发事务导致的问题

脏读：读到别人还没有提交的数据

幻读：之前读时没有（有）的数据，突然有（没有）了    插入或者删除

不可重复读：读出来的数据再次读发现数据不一样了

幻读和不可重复读的区别：==幻读是删除或者插入 不可重复读是更新==

![image-20220306224251779](https://cdn.jsdelivr.net/gh/Jason-Wu-1999/blog.img/imgs/image-20220306224251779.png)

### 如何解决幻读

1. 在RR隔离级别下可以直接使用MVCC，
2. 在RC隔离级别下 使用间隙锁

n行数据有n+1个间隙



**mvcc在RC和RR隔离级别下有readview的区别**

- RR下readview的生成：开启==事务之后第一个select语句==生成
- RC下readview的生成，开启事务之后==每一个select语句语==句开始时，都会重新将当前系统中的所有的活跃事务拷贝到一个列表生成

## [MVCC](D:\Coding\笔记\数据库\MySQL笔记\MVCC.md)

## 索引

### 一、哪些情况下适合建索引

　　1. 频繁作为where条件语句查询的字段

　　2. 关联字段需要建立索引，例如外键字段，student表中的classid,  classes表中的schoolid 等

　　3. 经常用于 `GROUP BY` 和 `ORDER BY` 的字段，这样在查询的时候就不需要再去做一次排序了，因为我们都已经知道了建立索引之后在 B+Tree 中的记录都是排序好的

　　5. 统计字段可以建立索引，例如count(),max()

### 二、哪些情况下不适合建索引

　　1.频繁更新的字段不适合建立索引

　　2.where条件中用不到的字段不适合建立索引

　　3.表数据可以确定比较少的不需要建索引

　　4.数据重复且发布比较均匀的的字段不适合建索引（唯一性太差的字段不适合建立索引），例如性别，真假值

　　5. 参与列计算的列不适合建索引

### 为什么要使用索引？

减少存储引擎需要扫描的数据量，加快查询速度。
索引可以把随机I/O变为顺序I/O。
对索引结果进行排序以避免使用磁盘临时表。

### 索引失效

1.  **联合索引非最左匹配**

   1. 在联合索引的情况下，数据是按照索引第一列排序，第一列数据相同时才会按照第二列排序。

   2. 也就是说，如果我们想使用联合索引中尽可能多的列，查询条件中的各个列必须是联合索引中从最左边开始连续的列。如果我们仅仅按照第二列搜索，肯定无法走索引

   3. > 有一个比较特殊的查询条件：where a = 1 and c = 3 ，符合最左匹配吗？
      >
      > 这种其实严格意义上来说是属于索引截断，不同版本处理方式也不一样。
      >
      > - MySQL 5.5 的话，前面 a 会走索引，在联合索引找到主键值后，开始回表，到主键索引读取数据行，然后再比对 c 字段的值。
      >
      > - 从 MySQL 5.6 之后，有一个==**索引下推功能**==，可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。

2. **对索引使用左或者左右模糊匹配**（like %xx` 或者 `like %xx%）

   1. 相当于不满足最左匹配原则，不过这是在一个索引内的最左匹配

   2. > 特例： 并不是所有的 左迷糊查询都不能走索引的
      >
      > 如果数据库表中的字段只有主键+二级索引，那么即使使用了左模糊匹配，也不会走全表扫描（type=all），而是走全扫描二级索引树(type=index)。

3. **对索引使用函数**

   1. 因为索引保存的是索引字段的原始值，而不是经过函数计算后的值，自然就没办法走索引了

4. **对索引进行表达式计算**  

   1. ```
      explain select * from t_user where id + 1 = 10;
      注意 where id = 10 - 1  不算对索引进行计算
      ```

5. **对索引隐式类型转换**：如果索引字段是字符串类型，但是在条件查询中，输入的参数是整型的话，你会在执行计划的结果发现这条语句会走全表扫描

   1. > ==注意==：**MySQL 在遇到字符串和数字比较的时候，会自动把字符串转为数字，然后再进行比较**。

   2. ```
      phone是字符串  id是数字
      不会走索引 等效于对索引使用函数
      select * from t_user where phone = 1300000001;    
      select * from t_user where CAST(phone AS signed int) = 1300000001;
      
      会走索引
       select * from t_user where id = '1';
       select * from t_user where id = CAST("1" AS signed int);
      ```
   ```
      
      **CAST 函数是作用在了 phone 字段，而 phone 字段是索引，也就是对索引使用了函数！而前面我们也说了，对索引使用函数是会导致索引失效的**
   ```

6.  **where子句中的OR**

    在 WHERE 子句中，如果在 OR 前的条件列是索引列，而在 OR 后的条件列不是索引列，那么索引会失效。

    

> 在使用**范围查询**之后 会导致后面的索引失效
>
> 但是在MySQL 5.6 引入的**索引下推优化**（index condition pushdown)， **可以在联合索引遍历过程中，对联合索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数**。



### 建索引、使用索引要注意什么？

1. **数据类型越小越好。数据类型越小，在磁盘、内存和CPU缓存中需要的空间更少，处理速度更快。**
2. 数据类型越简单越好。整型优于字符串，内置日期和时间优于字符串。
3. 尽量避免NULL。
4. 复合索引将最常用作限制条件的列放在最左边，依次递减。
5. 复合索引中只要有一列含有NULL值，那么这一列对于此复合索引就是无效的。
6. 短索引。对串列进行索引，如果可能应该指定一个前缀长度，不仅可以提高查询速度而且可以节省磁盘空间和I/O操作。
7. mysql查询只使用一个索引，因此如果where子句中已经使用了索引的话，那么order by中的列是不会使用索引的。尽量不要包含多个列的排序，如果需要最好给这些列创建复合索引。
8. 建立索引的时候，可以加上nologging选项。以减少在建立索引过程中产生的大量redo，从而提高执行的速度。
9. 建立索引的时候要对表进行加锁，因此应当注意操作在业务空闲的时候进行。
10. 使用NOT IN和!=操作，mysql将无法使用索引。
11. 不要在列上进行运算，where字句的查询条件里使用了函数将不会使用索引。



### 查询优化

#### 小表驱动大表

> 小表驱动大表的主要目的是通过**减少表连接创建的次数，**加快查询速度 

- 对于left join 左边的是驱动表
- 对于right join 右边的是驱动表

主要原因：例如：现有两个表t1与t2 ，表t1有100条数据，表t2有20万条数据 ;

![image-20220707185018665](https://cdn.jsdelivr.net/gh/Jason-Wu-1999/blog.img//img/20220707185018.png)

可以看到，在这条语句里，被驱动表t2的字段a上有索引，join过程用上了这个索引，因此这个语 句的执行流程是这样的：

1. 从表t1中读入一行数据 R； 
2. 从数据行R中，取出a字段到表t2里去查找； 
3. 取出表t2中满足条件的行，跟R组成一行，作为结果集的一部分； 
4. 重复执行步骤1到3，直到表t1的末尾循环结束。

##### 总结：

> 被驱动表会进行==全表扫描==，被驱动表如果 连接条件的字段有索引的话，可以==使用索引==
>
> 所以使用小表作为驱动表可以减少扫描行数



#### limit 优化

```
select * from record limit 2000000,10
```



### 聚簇索引和非聚簇索引

#### **聚簇索引**：

- 数据和索引是存放在一起的，找到了索引就找到了数据
- **存储数据的顺序和索引顺序一致**
- 一张表只能有一个聚簇索引（一般主键会默认聚簇索引）

#### **非聚簇索引**：

- 非聚簇索引的叶子节点仍然是索引节点，只不过有指向对应数据块的指针

聚簇索引查询相对会更快一些，因为主键索引树的叶子节点直接就是我们要查询的整行数据了

### count函数

> 该函数作用是**统计符合查询条件的记录中，函数指定的参数不为 NULL 的记录有多少个**。
>
> 性能 ： count(*)  = count(1) > count(主键字段)  > count(非索引的普通字段)

- 你首先要明确的是，在不同的MySQL引擎中，count(*)有不同的实现方式。*
  - MyISAM引擎把一个表的总行数存在了磁盘上，因此执行count(*)的时候会直接返回这个数， 效率很高； 
  - 而InnoDB引擎就麻烦了，它执行count(*)的时候，需要把数据一行一行地从引擎里面读出 来，然后累积计数。

- count（1） ： InnoDB引擎遍历整张表，但不取值。server层对于返回的每一行，放一个 数字“1”进去，判断是不可能为空的，按行累加

- ```
  但是count(*)是例外，并不会把全部字段取出来，而是专门做了优化，不取值。count(*)肯定不是null，按行累加
  ```

-  count(主键字段) 
  - InnoDB引擎会遍历整张表，把每一行的主键id值都取出来，返回给server 层。server层拿到id后，判断是不可能为空的，就按行累加。
  - 在通过 count 函数统计有多少个记录时，MySQL 的 server 层会维护一个名叫 count 的变量。
- server 层会循环向 InnoDB 读取一条记录，如果 count 函数指定的参数不为 NULL，那么就会将变量 count 加 1，直到符合查询的全部记录被读完，就退出循环。最后将 count 变量的值发送给客户端。
  
- count(普通字段)：会进行全表扫描 



==count(1)、 count(*)、 count(主键字段)在执行的时候，如果表里存在二级索引，优化器就会选择二级索引进行扫描。==

这是**因为相同数量的二级索引记录可以比聚簇索引记录占用更少的存储空间**，所以二级索引树比聚簇索引树小，这样遍历二级索引的 I/O 成本比遍历聚簇索引的 I/O 成本小，因此「优化器」优先选择的是二级索引。



- MyISAM表虽然count(*)很快，但是不支持事务；

* show table status命令虽然返回很快，但是不准确；
*  InnoDB表直接count(*)会遍历全表，虽然结果准确，但会导致性能问题。



因为count（*）会影响性能

#### 如果你现在有一个页面经常要显示交易系统的操作记录总数，到底应该怎么办呢？

> 答案是，我们只能自己计数

方法一：使用redis缓存计数  **存在问题**， 会存在不一致的问题

方法二：使用MySQL计数  利用事务 ==把插入数据和增加计数放到同一个事务中==

把计数放在Redis里面，不能够保证计数和MySQL表里的数据精确一致的原因，**是这两个 ==不同的存储构成的系统==，不支持分布式事务，无法拿到精确一致的视图**。而把计数值也放在 MySQL中，就解决了一致性视图的问题。



### hash索引和B+树索引的优缺点

- Hash索引 

- **优点：**

  ​		Hash索引的检索可以一次到位，所以Hash索引的查询效率更高。 ==O(1)==

- **缺点：** 

  1. Hash索引只能满足"="，“IN”，"!="，==不能使用范围查询。==
  2. Hash值的大小关系不一定个原键值一样，==不能做排序操作。==
  3. 联合索引中不能利用部分索引键查询。
  4. ==遇到大量Hash值相等的情况后，性能不一定比B树高。==
  5. 存储引擎会为Hash索引中的每一列都计算hash码，Hash索引中存储的即hash码，所以==每次读取都会进行两次查询。==

- InnoDB引擎有一个特殊功能叫“自适应哈希索引”。当InnoDB发现某些索引值被使用的非常频繁是，会在内存中基于B-Tree索引之上再建一个哈希索引，这样可以让B-Tree索引具有哈希索引的优点。这是一个完全自动的、内部的行为，用户无法控制或配置（如果有必要，可以关闭该功能）。

**B+树索引**（平衡多叉树）:优化的b树索引

- 优点：
  - ==所有索引数据都在叶子结点上，==非叶子节点占用内存更小，可以让树的高度更矮，减少IO次数
  - 并且==增加了顺序访问指针==,每个叶子节点都有指向相邻叶子节点的指针。范围查询效率更高，**b树**的范围查询只能通过树的遍历来完成范围查询
  - **利用了磁盘预读原理**，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入。每次读取多少读取一个页的数据
  - **InnoDB 的数据是按「数据页」为单位来读写的**，也就是说，当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。
  - B-Tree中一次检索最多需要h-1次I/O（根节点常驻内存），渐进复杂度为O(h)=O(logdN)。
  - 假设叶子节点页可以存放100条记录，内部节点可以存放1000条目录，则
  - h=1，最多能放100条记录。
  - h=2，最多能放1000*100=100000条记录。
  - h=3，最多能放1000*1000*100=100000000（一亿）条记录。
  - h=4，最多能放1000*1000*1000*100=100000000000（一千亿）条记录，应该不会有这种数据量的表吧！
  - b+树的查询时间复杂度
    - m叉树，N个叶子几点
    -  访问的节点数量 = B+树深度 = logmN
  



### 如何计算B+树可以存放多少条数据

#### 叶子节点

![image-20220515103618472](https://cdn.jsdelivr.net/gh/Jason-Wu-1999/blog.img//imgsimage-20220515103618472.png)

- B+树 的一个节点大小=innodb的一页=4个操作系统页(一页4kb)===16kb(==系统规定，不用纠结)

- 叶子节点只存储数据(索引值和链表指针占不考虑)

  - 假设一行数据=1kb(1kb大概有几千上万个字符，所以按1kb算一行数据不算少了)

    那么按一个索引值来算：**叶子节点可以存放=1*16=16 条数据**

- 非叶子节点存储(索引值+指针)=8b(以bigint类型为例)+6b(指针大小为6个字节)=14b

  - **非叶子节点(索引值+指针)=一个节点的数据/14b=16kb/14b=16*1024 / 14 = 1170 个 (索引值+指针)**

> 一个节点 16k，一个节点可以接1170个节点，一个叶子节点可以存放16条数据



高度为2的b+树可以存放 `1170*16=18724`条数据

高度为3  ：`1770*1770*16`   **千万级别的数据**



### 为什么使用b+树，不使用红黑树

红黑树是平衡二叉树，树的高度会很高，IO次数很多



### 为什么MySQL使用b+树，而不用跳表

==**b+树的查询效率更高**，但是实现和维护比跳表复杂==

- B+树是多叉平衡搜索树，扇出高，只需要3层左右就能存放2kw左右的数据，同样情况下跳表则需要24层左右，假设层高对应**磁盘IO**，那么B+树的读性能会比跳表要好，因此mysql选了B+树做索引。

### 为什么Redis使用跳表，不使用b+树

- redis的读写全在内存里进行操作，**不涉及磁盘IO**，同时**跳表实现简单，相比B+树、AVL树、==少了旋转树结构的开销==**，因此redis使用跳表来实现ZSET，而不是树结构。



### 覆盖索引和回表

如果某个查询语句使用了二级索引，但是查询的数据不是主键值，这时在二级索引找到主键值后，需要去聚簇索引中获得数据行，这个过程就叫作「回表」，**也就是说要查两个 B+ 树才能查到数据**。不过，当查询的数据是主键值时，因为只在二级索引就能查询到，不用再去聚簇索引查，这个过程就叫作「索引覆盖」，也就是只需要查一个 B+ 树就能找到数据。

#### 所以不用主键不一定会回表，有可能是覆盖索引。

### 前缀索引

- 前缀索引可以减小索引项的大小 
- order by 就无法使用前缀索引
- 无法把前缀索引用作覆盖索引



#### 前缀区分度不够的情况怎么办？ 如身份证号

- hash
- 倒序

它们的区别，主要体现在以下三个方面： 

1. 从占用的额外空间来看，倒序存储方式在主键索引上，不会消耗额外的存储空间，而hash字 段方法需要增加一个字段。当然，倒序存储方式使用4个字节的前缀长度应该是不够的，如 果再长一点，这个消耗跟额外这个hash字段也差不多抵消了。 
2. 在CPU消耗方面，倒序方式每次写和读的时候，都需要额外调用一次reverse函数，而hash 字段的方式需要额外调用一次crc32()函数。如果只从这两个函数的计算复杂度来看的 话，reverse函数额外消耗的CPU资源会更小些。 
3.  从查询效率上看，使用hash字段方式的查询性能相对更稳定一些。因为crc32算出来的值虽 然有冲突的概率，但是概率非常小，可以认为每次查询的平均扫描行数接近1。而倒序存储 方式毕竟还是用的前缀索引的方式，也就是说还是会增加扫描行数。





## null和空值的区别

- MySQL中，null是未知的，==且占用空间的==。null使得索引、索引统计和值都更加复杂，并且影响优化器的判断。
- 空值('')是不占用空间的，注意空值的''之间是没有空格。
- 在进行count()统计某列的记录数的时候，如果采用的 NULL 值，会被系统自动忽略掉，**==但是空值是会进行统计到其中的。==**
- 判断null使用is null或者is not null，但判断空字符使用 =''或者 <>''来进行处理。
- 对于timestamp数据类型，如果插入 NULL 值，则出现的值是当前系统时间。插入空值，则会出现'0000-00-00 00:00:00'  。
- 对于已经创建好的表，普通的列将 null修改为 not null带来的性能提升比较小，所以调优时没有必要特意一一查找并null修改 not null。
- 对于已经创建好的表，如果计划在列上创建索引，那么尽量修改为not null，并且使用0 或者一个特殊值或者空值''。



> 索引列中有null是可以的，**Null值的列也是走索引的 ** 但是一般情况不建议使用，使用not null 没用值的时候使用默认值**考虑使用 0、特殊值或空字符串来代替它**

NULL值会有不少坑
1、count(字段NULL)会过滤统计的数据，sum这些函数也会
2、使用> < 的时候也会过滤掉为NULL的数据
3、group by 的时候会把所有为NULL的数据合并，可以随机生成UUID解决
4、还有场景可能也有问题，这里我也忘记了，用的时候才会想起来。



## [锁](https://xiaolincoding.com/mysql/lock/mysql_lock.html)



### 全局锁



### 表级锁

#### 表锁 

#### 元数据锁 

- 不需要显示加，会自动加
- **MDL的作用是，保证读写的正确性。**你可以想象一下，如果一个查询正在遍历一个 表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果 跟表结构对不上，肯定是不行的。
- 当对一个表做==增删改查==操作的时候，加MDL读锁
- 当要对==表做结构变更==操作的时候，加MDL写锁。



### 行级锁

> 实现原理： InnoDB行锁是通过给索引项加锁来实现的

- Record Lock，记录锁，也就是仅仅把一条记录锁上；
- Gap Lock，间隙锁，锁定一个范围，但是不包含记录本身；**（开区间）**
- Next-Key Lock：Record Lock + Gap Lock 的组合，锁定一个范围，并且锁定记录本身。

**MySQL 8.0.26 版本的行级锁的加锁规则。**

> 原则1：加锁的基本单位是next-key lock。希望你还记得，next-key lock是前开后闭区间。
>
> 原则2：查找过程中访问到的对象才会加锁。
>
> 优化1：索引上的等值查询，给唯一索引加锁的时候，next-key lock退化为行锁。 
>
> 优化2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock 退化为间隙锁。 
>
> 一个bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止。

唯一索引等值查询：

- 当查询的记录是存在的，next-key lock 会退化成「记录锁」。
- 当查询的记录是不存在的，next-key lock 会退化成「间隙锁」。

非唯一索引等值查询：

- 当查询的记录存在时，除了会加 next-key lock 外，还额外加间隙锁，也就是会加两把锁。
- 当查询的记录不存在时，只会加 next-key lock，然后会退化为间隙锁，也就是只会加一把锁。

非唯一索引和主键索引的范围查询的加锁规则不同之处在于：

- 唯一索引在满足一些条件的时候，next-key lock 退化为间隙锁和记录锁。
- 非唯一索引范围查询，next-key lock 不会退化为间隙锁和记录锁。

这些加锁规则其实很好总结的，大家自己可以用我文中的案例测试一遍，看一下你的 MySQL 版本和我的 MySQL 版本的加锁规则有什么不同。****



### update注意事项

update语句时，需要加索引

> 当我们执行 update 语句时，实际上是会对记录加独占锁（X 锁）的，如果其他事务对持有独占锁的记录进行修改时是会被阻塞的。，这个锁并不是执行完 update 语句就会释放的，而是会==等事务结束时才会释放。==

那 update 语句的 where 带上索引就能避免全表记录加锁了吗？

并不是。

**关键还得看这条语句在执行过程种，优化器最终选择的是索引扫描，还是全表扫描，如果走了全表扫描，就会对全表的记录加锁了**。



### select......for update会锁表还是锁行？

elect查询语句是不会加锁的，但是`select .......for update`除了有查询的作用外，还会加锁呢，而且它是**悲观锁**。

那么它加的是行锁还是表锁，这就要看是不是用了索引/主键。

没用索引/主键的话就是表锁，否则就是是行锁。

> ==**update和select......for update都会加独占锁**== 情况相同
>
> **UPDATE、DELETE、INSERT、SELECT ... LOCK IN SHARE MODE、SELECT ... FOR UPDATE**这些操作都是一种当前读 都会加独占锁



### MySQL中如何避免死锁？

死锁的四个必要条件：**互斥、占有且等待、不可强占用、循环等待**。只要系统发生死锁，这些条件必然成立，但是只要破坏任意一个条件就死锁就不会成立。

在数据库层面，有两种策略通过「打破循环等待条件」来解除死锁状态：

- **设置事务等待锁的超时时间**。当一个事务的等待时间超过该值后，就对这个事务进行回滚，于是锁就释放了，另一个事务就可以继续执行了。在 InnoDB 中，参数 `innodb_lock_wait_timeout` 是用来设置超时时间的，默认值时 50 秒。

  当发生超时后，就出现下面这个提示：

![图片](https://cdn.jsdelivr.net/gh/Jason-Wu-1999/blog.img//img/20220620212040.png)

- **开启主动死锁检测**。主动死锁检测在发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 `innodb_deadlock_detect` 设置为 on，表示开启这个逻辑，默认就开启。

  当检测到死锁后，就会出现下面这个提示：

![图片](https://cdn.jsdelivr.net/gh/Jason-Wu-1999/blog.img//img/20220620212044.png)

上面这个两种策略是「当有死锁发生时」的避免方式。

> 我们可以回归业务的角度来预防死锁，对订单做幂等性校验的目的是为了保证不会出现重复的订单，那我们可以直接将 order_no 字段设置为唯一索引列，利用它的唯一下来保证订单表不会出现重复的订单，不过有一点不好的地方就是在我们插入一个已经存在的订单记录时就会抛出异常。



## 日志

**MySQL的三个重要日志undolog  redolog binlog**

==怎么保持原子性和持久性  （redo和undo日志文件）两个日志文件什么时候写的日志文件==

- **undo log**名为回滚日志，是实现原子性的关键，当事务回滚时能够撤销所有已经成功执行的sql语句，他需要记录你要回滚的相应日志信息。
- **持久性是利用了redo log**。Mysql是先把磁盘上的数据加载到内存中，在内存中对数据进行修改，再刷回磁盘上。如果此时突然宕机，内存中的数据就会丢失。redo log包括两部分：一是内存中的日志缓冲(redo log buffer)，该部分日志是易失性的；二是磁盘上的重做日志文件(redo log file)，该部分日志是持久的。innodb通过force log at commit机制实现事务的持久性，**即在事务提交的时候，必须先将该事务的所有事务日志写入到磁盘上的redo log file和undo log file中进行持久化。也就是说提交了两个日志文件。

### MySQL的WAL机制

WAL全称为Write-Ahead Logging，**预写日志系统**。其主要是**==指MySQL在执行写操作的时候并不是立刻更新到磁盘上，而是先记录在日志中，之后在合适的时间更新到磁盘中==**。日志主要分为undo log、redo log、binlog。
当内存数据页跟磁盘数据页内容不一致的时候，我们成这个内存页为“脏页”。内存数据写入磁盘后，内存和磁盘上的数据页内容就一致了，称为“干净页”。
MySQL真正使用WAL的原因是：磁盘的写操作是随机IO，比较耗性能，所以如果把每一次的更新操作都先写入log中，那么就成了顺序写操作，实际更新操作由后台线程再根据log异步写入。这样对于client端，延迟就降低了。并且，由于顺序写入大概率是在一个磁盘块内，这样产生的IO次数也大大降低。所以WAL的核心在于将随机写转变为了顺序写，降低了客户端的延迟，提升了吞吐量。



### Undolog

两个作用：

- **实现事务回滚，保障事务的原子性**。事务处理过程中，如果出现了错误或者用户执 行了 ROLLBACK 语句，MySQL 可以利用 undo log 中的历史数据将数据恢复到事务开始之前的状态。
- **实现 MVCC（多版本并发控制）关键因素之一**。MVCC 是通过 ReadView + undo log 实现的。undo log 为每条记录保存多份历史数据，MySQL 在执行快照读（普通 select 语句）的时候，会根据事务的 Read View 里的信息，顺着 undo log 的版本链找到满足其可见性的记录。



undolog和redolog的区别

- redo log 记录了此次事务**「完成后」**的数据状态，记录的是更新之**「后」**的值；
- undo log 记录了此次事务**「开始前」**的数据状态，记录的是更新之**「前」**的值；

事务提交之前发生了崩溃，重启后会通过 undo log 回滚事务，事务提交之后发生了崩溃，重启后会通过 redo log 恢复事务，

### redo log 日志

- 属于引擎层的，是innoDB引擎特有的
- redo log包括两部分：一个是**内存中的日志缓冲**(redo log buffer)，另一个**是磁盘上的日志文件**(redo log file)。
-  InnoDB 的 redo log 是**固定大小**的，比如我们配置了一组 4 个文件，每个文件大小是 1GB，redo log日志是进行循环写的  

![在这里插入图片描述](https://cdn.jsdelivr.net/gh/Jason-Wu-1999/blog.img/imgs/202009141715408.png)

**redo log是恢复在内存更新后，还没来得及刷到磁盘的数据。**



#### redo log 的整体流程

![img](https://cdn.jsdelivr.net/gh/Jason-Wu-1999/blog.img/imgs/20210726160442101.png)

以更新事务为例。

- 将原始数据读入内存，修改数据的内存副本。
- 生成redo log并写入重做日志缓冲区，redo log中存储的是修改后的新值。
- 事务提交时，将重做日志缓冲区中的内容刷新到重做日志文件。
- 随后正常将内存中的脏页刷回磁盘。

#### redo log的刷盘时机

在完成数据的修改之后，脏页刷入磁盘之前写入重做日志缓冲区。即先修改，再写入。

脏页：内存中与磁盘上不一致的数据（并不是坏的！）

在以下情况下，redo log由重做日志缓冲区写入磁盘上的重做日志文件。

- **redo log buffer的日志占据redo log buffer总容量的一半时，将redo log写入磁盘。**
- 一个事务提交时，他的redo log都刷入磁盘，这样可以保证数据绝不丢失（最常见的情况）。注意这时内存中的脏页可能尚未全部写入磁盘。
- **后台线程定时刷新，有一个后台线程每过一秒就将redo log写入磁盘。**
- **MySQL关闭时，redo log都被写入磁盘。**

#### redo log 的刷盘策略

第一种情况和第四种情况一定会执行redo log的写入，第二种情况和第三种情况的执行要根据参数`innodb_flush_log_at_trx_commit`的设定值，

（1）**设置为0** ：表示每次事务提交时不进行刷盘操作。默认master thread每隔1s进行一次重做日志的同步。

（2）**设置为1** ：表示每次事务提交时都将进行同步，刷盘操作（ 默认值 ）

（3）**设置为2** ：表示每次事务提交时都只把 redo log buffer 内容写入 page cache，不进行同步。由os自己决定什么时候同步到磁盘文件。

索引的创建也需要记录redo log。

![img](https://cdn.jsdelivr.net/gh/Jason-Wu-1999/blog.img//img/20220622111311.png)



#### redo log(重做日志) 如何保证事务的持久性？

InnoDB作为MySQL的存储引擎，数据是存放在磁盘中的，**但如果每次读写数据都需要磁盘IO，效率会很低**。为此，InnoDB提供了缓存(Buffer Pool)，**Buffer Pool中包含了磁盘中部分数据页的映射**，作为访问数据库的缓冲：当从数据库读取数据时，会首先从Buffer Pool中读取，如果Buffer Pool中没有，则从磁盘读取后放入Buffer Pool；当向数据库写入数据时，会首先写入Buffer Pool，Buffer Pool中修改的数据会定期刷新到磁盘中（这一过程称为刷脏）。

Buffer Pool的使用大大提高了读写数据的效率，但是也带了新的问题：**如果MySQL宕机，而此时Buffer Pool中修改的数据还没有刷新到磁盘，就会导致数据的丢失，事务的持久性无法保证。**

于是，**redo log** 被引入来解决这个问题：当数据修改时，除了修改Buffer Pool中的数据，还会在 redo log 记录这次操作；当事务提交时，会调用fsync接口对redo log进行刷盘。如果MySQL宕机，重启时可以读取redo log中的数据，对数据库进行恢复。redo log采用的是WAL（Write-ahead logging，预写式日志），所有修改先写入日志，再更新到Buffer Pool，保证了数据不会因MySQL宕机而丢失，从而满足了持久性要求。

> **既然 redo log 也需要在事务提交时将日志写入磁盘，为什么它比直接将Buffer Pool中修改的数据写入磁盘(即刷脏)要快呢？主要有以下两方面的原因：**

（1）刷脏是随机IO，因为每次修改的数据位置随机，**但写redo log是==追加操作，属于顺序IO==。**

（2）刷脏是**==以数据页（Page）为单位的==**，MySQL默认页大小是16KB，一个Page上一个小修改都要整页写入；而==redo log中只包含真正需要写入的部分，无效IO大大减少。==

<img src="https://cdn.jsdelivr.net/gh/Jason-Wu-1999/blog.img/imgs/61aa9e96c24548a6a43cd3cc525a7996.png" alt="在这里插入图片描述" style="zoom:50%;" />

#### redo log 要写到磁盘，数据也要写磁盘，为什么要多此一举？

写入 redo log 的方式使用了追加操作， 所以磁盘操作是**顺序写**，而写入数据需要先找到写入位置，然后才写到磁盘，所以磁盘操作是**随机写**。

磁盘的「顺序写 」比「随机写」 高效的多，因此 redo log 写入磁盘的开销更小。

针对「顺序写」为什么比「随机写」更快这个问题，可以比喻为你有一个本子，按照顺序一页一页写肯定比写一个字都要找到对应页写快得多。

可以说这是 WAL 技术的另外一个优点：**MySQL 的写操作从磁盘的「随机写」变成了「顺序写」**，提升语句的执行性能。这是因为 MySQL 的写操作并不是立刻更新到磁盘上，而是先记录在日志上，然后在合适的时间再更新到磁盘上 。

至此， 针对为什么需要 redo log 这个问题我们有两个答案：

- **实现事务的持久性，让 MySQL 有 crash-safe 的能力**，能够保证 MySQL 在任何时间段突然崩溃，重启后之前已提交的记录都不会丢失；
- **将写操作从「随机写」变成了「顺序写」**，提升 MySQL 写入磁盘的性能。



### MySQL的binlog日志

- `binlog`属于server层，不属于任何的存储引擎
- ==binlog日志不记录查询操作==，

使用直接追加的方式，超过有效时间才会删除，如果超过单日志的最大值（默认1G，可以通过变量 max_binlog_size 设置），则会新起一个文件继续记录。但由于日志可能是基于事务来记录的(如InnoDB表类型)，而事务是绝对不可能也不应该跨文件记录的，如果正好binlog日志文件达到了最大值但事务还没有提交则不会切换新的文件记录，而是继续增大日志，所以 max_binlog_size 指定的值和实际的binlog日志大小不一定相等

- 如何生成的？：**通过追加的方式**

#### binlog刷盘时机

对于InnoDB存储引擎而言，==只有在事务提交时才会记录biglog==，此时记录还在内存中，那么biglog是什么时候刷到磁盘中的呢？sync_binlog参数控制biglog的刷盘时机，**取值范围是0-N：**

在事务提交的时候，执行器把 binlog cache 里的完整事务写入到 binlog 文件中，并清空 binlog cache。如下图：

![binlog cach](https://cdn.jsdelivr.net/gh/Jason-Wu-1999/blog.img//img/20220627222826.png)

虽然每个线程有自己 binlog cache，但是最终都写到同一个 binlog 文件：

- 图中的 write，指的就是指把日志写入到 binlog 文件，但是并没有把数据持久化到磁盘，因为数据还缓存在文件系统的 page cache 里，write 的写入速度还是比较快的，因为不涉及磁盘 I/O。
- 图中的 fsync，才是将数据持久化到磁盘的操作，这里就会涉及磁盘 I/O，所以频繁的 fsync 会导致磁盘的 I/O 升高。

**MySQL提供一个 sync_binlog 参数来控制数据库的 binlog 刷到磁盘上的频率：**

- sync_binlog = 0 的时候，表示每次提交事务都只 write，不 fsync，后续交由操作系统决定何时将数据持久化到磁盘；
- sync_binlog = 1 的时候，表示每次提交事务都会 write，然后马上执行 fsync；
- sync_binlog =N(N>1) 的时候，表示每次提交事务都 write，但累积 N 个事务后才 fsync。



#### binlog和redo log的区别

1、redo log是innoDB引擎特有的；binlog是MySQL的Server实现的，所有引擎都可以使用

2、redo log是**物理日志**，记录的是“在某个数据页上做了什么修改”；binlog是记录的这个语句的原始逻辑(**逻辑日志**)，比如“给D=2这一行的C字段加1”。

3、redo log**是循环写**的，空间固定会用完；binlog是可以**追加写入的**。“追加写“是指binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。

4、**用途不同**： binlog 用于**备份恢复**、**主从复制**；   redo log 用于**掉电等故障恢复。**

假设一个事务，对表做了10万行的记录插入，在这个过程中，一直不断的往redo log顺序记录，而bin log不会记录，直到这个事务提交，才会一次写入bin log文件中。

有了这两个日志之后，我们来看一下一条更新语句是怎么执行的（redo 不能一次写入了）：

![img](https://cdn.jsdelivr.net/gh/Jason-Wu-1999/blog.img/imgs/202212983614921.png)

> 你能发现 redo log 竟然是先 prepare 状态，等 binlog 写完之后，才是 commit 状态，这种方式就叫”两阶段提交”。为什么会有这种方式呢？

redo log 和 binlog 都可以用于表示事务的提交状态，而`两阶段提交`就是让这两个状态保持逻辑上的一致。

#### redo log 和 binlog是怎么关联起来的? 

回答：它们有一个共同的数据字段，叫XID。崩溃恢复的时候，会按顺序扫描redo log： 如果碰到既有prepare、又有commit的redo log，就直接提交； 如果碰到只有parepare、而没有commit的redo log，就拿着XID去binlog找对应的事务。如果binlog中没有说明之前事务提交失败不需要恢复。



#### 如果不小心整个数据库的数据被删除了，能使用 redo log 文件恢复数据吗？

不可以使用 redo log 文件恢复，**只能使用 binlog 文件恢复。**

因为 redo log 文件是循环写，是会边写边擦除日志的，只记录未被刷入磁盘的数据的物理日志，已经刷入磁盘的数据都会从 redo log 文件里擦除。

binlog 文件保存的是全量的日志，也就是保存了所有数据变更的情况，理论上只要记录在 binlog 上的数据，都可以恢复，所以如果不小心整个数据库的数据被删除了，得用 binlog 文件恢复数据。

#### 脏页刷新

下面几种情况会触发脏页的刷新：

- 当 redo log 日志满了的情况下，会主动触发脏页刷新到磁盘；
- Buffer Pool 空间不足时，需要将一部分数据页淘汰掉，如果淘汰的是脏页，需要先将脏页同步到磁盘；
- MySQL 认为空闲时，后台线程回定期将适量的脏页刷入到磁盘；
- MySQL 正常关闭之前，会把所有的脏页刷入到磁盘；

#### 执行一条更新语句的过程

![img](https://cdn.jsdelivr.net/gh/Jason-Wu-1999/blog.img//img/20220622100732.png)

<img src="https://cdn.jsdelivr.net/gh/Jason-Wu-1999/blog.img//img/20220622121131.png" alt="image-20220622121130671" style="zoom:50%;" />

1. 执行器负责具体执行，会调用存储引擎的接口，通过主键索引树搜索获取 id = 1 这一行记录：
   - 如果 id=1 这一行所在的数据页本来就在 buffer pool 中，就直接返回给执行器更新；
   - 如果记录不在 buffer pool，将数据页从磁盘读入到 buffer pool，返回记录给执行器。
2. 执行器得到聚簇索引记录后，会看一下更新前的记录和更新后的记录是否一样：
   - 如果一样的话就不进行后续更新流程；
   - 如果不一样的话就把更新前的记录和更新后的记录都当作参数传给 InnoDB 层，让 InnoDB 真正的执行更新记录的操作；
3. 开启事务， InnoDB 层更新记录前，首先要记录相应的 undo log，因为这是更新操作，需要把被更新的列的旧值记下来，也就是要生成一条 undo log，undo log 会写入 Buffer Pool 中的 Undo 页面，不过在修改该 Undo 页面前需要先记录对应的 redo log，所以**先记录修改 Undo 页面的 redo log ，然后再真正的修改 Undo 页面**。
4. InnoDB 层开始更新记录，根据 WAL 技术，**先记录修改数据页面的 redo log ，然后再真正的修改数据页面**。修改数据页面的过程是修改 Buffer Pool 中数据所在的页，然后将其页设置为脏页，为了减少磁盘I/O，不会立即将脏页写入磁盘，后续由后台线程选择一个合适的时机将脏页写入到磁盘。
5. 至此，一条记录更新完了。
6. 在一条更新语句执行完成后，然后开始记录该语句对应的 binlog，此时记录的 binlog 会被保存到 binlog cache，并没有刷新到硬盘上的 binlog 文件，在事务提交时才会统一将该事务运行过程中的所有 binlog 刷新到硬盘。
7. 事务提交，剩下的就是「两阶段提交」的事情了，接下来就讲这个



## 主从复制

![MySQL 主从复制过程](https://cdn.jsdelivr.net/gh/Jason-Wu-1999/blog.img//img/20220622111551.png)

MySQL 集群的主从复制过程梳理成 3 个阶段：

- **写入 Binlog**：主库写 binlog 日志，提交事务，并更新本地存储数据。
- **同步 Binlog**：把 binlog 复制到所有从库上，每个从库把 binlog 写到暂存日志中。
- **回放 Binlog**：回放 binlog，并更新存储引擎中的数据。

> MySQL 主从复制还有哪些模型？

主要有三种：

- **同步复制**：MySQL 主库提交事务的线程要等待所有从库的复制成功响应，才返回客户端结果。**这种方式在实际项目中，基本上没法用，**原因有两个：一是性能很差，因为要复制到所有节点才返回响应；二是可用性也很差，主库和所有从库任何一个数据库出问题，都会影响业务。
- **异步复制**（默认模型）：MySQL 主库提交事务的线程并不会等待 binlog 同步到各从库，就返回客户端结果。这种模式一旦主库宕机，数据就会发生丢失。
- **半同步复制**：MySQL 5.7 版本之后增加的一种复制方式，介于两者之间，事务线程不用等待所有的从库复制成功响应，只要一部分复制成功响应回来就行，比如一主二从的集群，只要数据成功复制到任意一个从库上，主库的事务线程就可以返回给客户端。这种**半同步复制的方式，兼顾了异步复制和同步复制的优点，即使出现主库宕机，至少还有一个从库有最新的数据，不存在数据丢失的风险**



### ==数据库和缓存怎么保持一致性==?

#### 删除缓存策略

每次有写请求改变数据库的值的时候，

1. 先将缓存删掉，
2. 再把数据库中的值更新，
3. 等下一个读请求过来，发现没有缓存会访问数据库，并放入缓存中

**存在的问题**：在高并发的情况下，假设，在删除缓存之后，还没有更新缓存之前，来了一个读请求，读的还是数据库中之前的数据,并把他放进缓存中导致   缓存和数据库中数据不一致



#### 延时双删策略

1. 先将缓存删掉，
2. 再把数据库中的值更新，
3. 延时500ms
4. 再次删除缓存
5. 等下一个读请求过来，发现没有缓存会访问数据库，并放入缓存中

这样可以即使出现了上面的情况，这个写请求也会再次删除缓存 ，等下一个读请求过来，会拿到数据库中更新完的数据放到缓存中



**存在的问题**：在没有读写分离的情况下有问题

![image-20220306223045686](https://cdn.jsdelivr.net/gh/Jason-Wu-1999/blog.img/imgs/image-20220306223045686.png)

因为是从 从库读取数据，有可能主从同步的时候出现问题，导致读取不到，而导致  缓存和数据库不一样

#### 改进方法

#### redis订阅mysql的binlog日志

![image-20220306223951588](https://cdn.jsdelivr.net/gh/Jason-Wu-1999/blog.img/imgs/image-20220306223951588.png)



### mysql的主从复制、读写分离

#### 主从复制的原理

![img](https://cdn.jsdelivr.net/gh/Jason-Wu-1999/blog.img/imgs/mysql%E4%B8%BB%E4%BB%8Efuzh.png)

- master在进行增删改操作时，会将其按顺序记录在**二进制日志binlog**文件中
- slave 开启两个线程-->==IO 线程（读写）==到master的binary log **dump线程**binary log(二进制日志上面去拿取数据
- 拿取二进制 binary log 到从（salve)上**写入Relay log**中
- ==SQL Thread== 会从 relay log上获取上次读取到的位置点 --->SQL thread 再从 Relay log（中继日志） 上读取数据从而保持数据的一致。

### 读写分离可能出现的问题

读写分离，由于**能分担主库的压力**，很多情况会考虑读写分离。但是在使用时，就应该考虑到一些问题，其中最主要的就是**主从延迟**。这个就看业务是否能接受延迟了。==如果不能接受延迟，建议采用半同步复制并且加上延迟判断==。存在延迟则把读请求放到主库，没延迟就读从库。如果业务能接受延迟，可以等数据同步完成，再去从库进行查询。

#### 半同步复制

- 在主库开启 binlog 的情况下

- 如果主库有增删改的语句，会记录到 binlog 中

- 主库通过 IO 线程把 binlog 里面的内容传给从库的中继日志(relay log)中

- 从库收到 binlog 后，发送给主库一个 ACK，表示收到了

- ==主库收到这个 ACK 以后，才能给客户端返回 commit 成功==，相当于commit的时候从库已经从主库那里拿到binlog了
- 从库的 SQL 线程负责读取它的 relay log 里的信息并应用到从库数据库中

![c568c255c3e5c9b00615203e5bf0e4b3.png](https://cdn.jsdelivr.net/gh/Jason-Wu-1999/blog.img/imgs/%E5%8D%8A%E5%90%8C%E6%AD%A5)

#### 主要问题：主从延时

解决方法 

- **延时判断**：先判断主从是否存在延迟，如果存在延迟，则查询落在主库，如果没延迟，则查询语句落在从库。
- **延时判断+半同步复制**

​	

## 如果主服务器挂了，如何让从服务器成为主服务器呢？

1. 停止向旧主库写入；
2. 让从库追赶上主库；
3. 将一台从库配置为新的主库；
4. 将从库与写操作指向新的主库，然后开启主库的写入。



### sql优化  https://zhuanlan.zhihu.com/p/299051996





# 内存

##  [BufferPool](https://xiaolincoding.com/mysql/buffer_pool/buffer_pool.html)

> 在 MySQL 启动的时候，**InnoDB 会为 Buffer Pool 申请一片连续的内存空间，然后按照默认的`16KB`的大小划分出一个个的页， Buffer Pool 中的页就叫做缓存页**。此时这些缓存页都是空闲的，之后随着程序的运行，才会有磁盘上的页被缓存到 Buffer Pool 中。

Buffer Pool 除了缓存「索引页」和「数据页」，还包括了 undo 页，插入缓存、自适应哈希索引、锁信息等等。

![img](https://cdn.jsdelivr.net/gh/Jason-Wu-1999/blog.img//img/20220628213612.png)

Innodb 通过三种链表来管理缓页：

- Free List （空闲页链表），管理空闲页；
- Flush List （脏页链表），管理脏页；
- LRU List，管理脏页+干净页，将最近且经常查询的数据缓存在其中，而不常查询的数据就淘汰出去。；

![img](https://cdn.jsdelivr.net/gh/Jason-Wu-1999/blog.img//img/20220628213659.png)



### 如何提高缓存的命中率

> LRU算法

简单的 LRU 算法并没有被 MySQL 使用，因为简单的 LRU 算法无法避免下面这两个问题：

- 预读失效；
- Buffer Pool 污染；

**LRU的优化算法**：将LRU链分成两部分，将预读的数据页放到后半部分

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/innodb/young%2Bold.png)

#### 什么是 Buffer Pool 污染？

当某一个 SQL 语句**扫描了大量的数据**时，在 Buffer Pool 空间比较有限的情况下，可能会将 **Buffer Pool 里的所有页都替换出去，导致大量热数据被淘汰了**，等这些热数据又被再次访问的时候，由于缓存未命中，就会产生大量的磁盘 IO，MySQL 性能就会急剧下降，这个过程被称为 **Buffer Pool 污染**。

注意， Buffer Pool 污染并不只是查询语句查询出了大量的数据才出现的问题，即使查询出来的结果集很小，也会造成 Buffer Pool 污染。

比如，在一个数据量非常大的表，执行了这条语句：

```sql
select * from t_user where name like "%xiaolin%";
```

可能这个查询出来的结果就几条记录，但是由于这条语句会发生索引失效，所以这个查询过程是全表扫描的，接着会发生如下的过程：

- 从磁盘读到的页加入到 LRU 链表的 old 区域头部；
- 当从页里读取行记录时，也就是页被访问的时候，就要将该页放到 young 区域头部；
- 接下来拿行记录的 name 字段和字符串 xiaolin 进行模糊匹配，如果符合条件，就加入到结果集里；
- 如此往复，直到扫描完表中的所有记录。

经过这一番折腾，原本 young 区域的热点数据都会被替换掉。

举个例子，假设需要批量扫描：21，22，23，24，25 这五个页，这些页都会被逐一访问（读取页里的记录）。

![img](https://cdn.jsdelivr.net/gh/Jason-Wu-1999/blog.img//img/20220628214200.png)

在批量访问这些数据的时候，会被逐一插入到 young 区域头部。

#### 如何解决

提高进入到young区的门槛